# -*- coding: utf-8 -*-
"""kenya_airways_analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LKx20SdlKaDy_mLCO8O9H5QkcNuV6XMX
"""

import pandas as pd
import matplotlib.pyplot as plt

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from scipy.special import softmax
import torch

plt.style.use('ggplot')

kq_data = pd.read_csv("sample_data/kenya-airways.csv")
kq_data.reset_index(inplace=True)
kq_data.rename(columns={'index':'id'}, inplace=True)
kq_data.head()

# retrieve the review related columns
# columns_to_keep = ["summary_title", "review", "ratings_10"]
# review_data = kq_data[columns_to_keep]
# review_data.head()

# check if there are null values
kq_data[['review', 'ratings_10']].isna().sum()

# handle null values
kq_data['full_review'] = kq_data['summary_title'] \
  .str.cat(kq_data['review'], sep=' ', na_rep='')

# remove redundant columns
# full_review = review_data.drop(columns=['summary_title', 'review'])
# full_review.head()

# visualizing the ratings distribution
ax = kq_data['ratings_10'].value_counts().sort_index() \
  .plot(kind='bar', figsize=(10, 6), title="Ratings Distribution")
ax.set_xlabel('Ratings')
ax.set_ylabel('Count')
plt.show()

example_review = kq_data['full_review'][300]
example_review

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

MODEL = f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL).to(device)

# run roberta model
encoded_text = tokenizer(example_review, return_tensors='pt')
encoded_text = {key: val.to(device) for key, val in encoded_text.items()}
with torch.no_grad():
  model_output = model(**encoded_text)

scores = model_output.logits[0].cpu().numpy()
scores = softmax(scores)

scores_dict = {
  'roberta_neg' : scores[0],
  'roberta_neu' : scores[1],
  'roberta_pos' : scores[2]
}
print(pd.Series(scores_dict))

# running on colab's gpu and if no gpu, revert to cpu
def sentiment_polarity_score(example_review):
  # tokenize the review
  encoded_text = tokenizer(example_review, return_tensors='pt',
                           padding='max_length', truncation=True,
                           max_length=512)
  # move the tokenized review to the same device as the model
  encoded_text = {key: val.to(device) for key, val in encoded_text.items()}
  # ensure no gradient computation done
  with torch.no_grad():
    model_output = model(**encoded_text)
  # get the logits and move them to the cpu for further processing
  scores = model_output[0][0].detach().cpu().numpy()
  scores = softmax(scores)
  # scores dictionary
  scores_dict = {
    'roberta_neg' : scores[0],
    'roberta_neu' : scores[1],
    'roberta_pos' : scores[2]
  }
  return pd.Series(scores_dict)  #scores_dict

# roberta_columns = ['roberta_neg', 'roberta_neu', 'roberta_pos']
try:
  kq_data[['roberta_neg', 'roberta_neu', 'roberta_pos']] = kq_data['full_review']\
    .apply(lambda x: sentiment_polarity_score(x))
except RuntimeError as e:
  print(e)

# from tqdm.notebook import tqdm
# roberta_results = {}
# for i, row in tqdm(kq_data.iterrows(), total=len(kq_data)):
#   text = row['full_review']
#   myid = row['id']
#   roberta_result = sentiment_polarity_score(text)
#   roberta_results[myid] = roberta_result
# print(roberta_results)

roberta_columns = ['roberta_neg', 'roberta_neu', 'roberta_pos']
kq_data['overall_sentiment'] = kq_data[roberta_columns].max(axis=1)

# Map column names to sentiment labels
sentiment_mapping = {
    'roberta_neg': 'negative',
    'roberta_neu': 'neutral',
    'roberta_pos': 'positive'
}
kq_data['sentiment_label'] = kq_data[roberta_columns].idxmax(axis=1)
kq_data['sentiment_label'] = kq_data['sentiment_label'].map(sentiment_mapping)
# kq_data['sentiment_label']
kq_data.head()

kq_data.to_csv("review_trained.csv", index=False)